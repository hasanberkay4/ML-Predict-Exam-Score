{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import random as rnd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import graphviz\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import time\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # Removing non-alphanumeric characters\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"<NUM>\", text) # Replacing numeric values with <NUM>\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    words = [word for word in words if word not in stop_words]  # Removing stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Data/dataset/*.html\"\n",
    "\n",
    "code2convos = dict()\n",
    "\n",
    "pbar = tqdm.tqdm(sorted(list(glob(data_path))))\n",
    "for path in pbar:\n",
    "    file_code = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "            \n",
    "        # Getting the file ID to use it as a key later on\n",
    "        fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "        # Reading the HTML file\n",
    "        html_page = fh.read()\n",
    "\n",
    "        # Parsing the HTML file with bs4\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "        # Grabbing the conversations with the data-testid pattern\n",
    "        data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "        conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "        convo_texts = []\n",
    "\n",
    "        for i, convo in enumerate(conversations):\n",
    "            convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "            if len(convo) > 0:\n",
    "                role = convo[0].get(\"data-message-author-role\")\n",
    "\n",
    "                # PREPROCESSING\n",
    "                text = convo[0].text\n",
    "                processed_text = preprocess_text(text)\n",
    "\n",
    "                convo_texts.append({\n",
    "                        \"role\" : role,\n",
    "                        \"text\" : processed_text\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        code2convos[file_code] = convo_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample entry\n",
    "pprint(code2convos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Prompts with Questions\n",
    "We matched the prompts with the questions in the homework using the code cells below. Firstly, for each student, the prompts were vectorized with a TF-IDF vectorizer, with the empty prompts marked and excluded along the way. Then, the same was done for the questions. A cosine similarity matrix was constructed using the two.\n",
    "\n",
    "The similarity matrix was used for outlier treatment. Codes (identifying students) associated with less than 15% similarity or contained empty prompts were eliminated.\n",
    "\n",
    "Finally, the treated data was vectorized with Word2Vec in 2 different ways: 1) per student and 2) as a whole, once. The resulting data was used to build another cosine similarity matrix. The final similarity matrix as well as the two Word2Vec vectors formed the basis the dataset we used for the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of user prompts\n",
    "prompts = []\n",
    "code2prompts = defaultdict(list)\n",
    "for code, convos in code2convos.items():\n",
    "    user_prompts = []\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            prompts.append(conv[\"text\"])\n",
    "            user_prompts.append(conv[\"text\"])\n",
    "    code2prompts[code] = user_prompts\n",
    "\n",
    "pprint(code2prompts[\"0031c86e-81f4-4eef-9e0e-28037abf9883\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"Initialize\n",
    "*   First make a copy of the notebook given to you as a starter.\n",
    "*   Make sure you choose Connect form upper right.\n",
    "*   You may upload the data to the section on your left on Colab, than right click on the .csv file and get the path of the file by clicking on \"Copy Path\". You will be using it when loading the data.\n",
    "\n",
    "\"\"\",\n",
    "#####################\n",
    "    \"\"\"Load training dataset (5 pts)\n",
    "    *  Read the .csv file with the pandas library\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Understanding the dataset & Preprocessing (15 pts)\n",
    "Understanding the Dataset: (5 pts)\n",
    "> - Find the shape of the dataset (number of samples & number of attributes). (Hint: You can use the **shape** function)\n",
    "> - Display variable names (both dependent and independent).\n",
    "> - Display the summary of the dataset. (Hint: You can use the **info** function)\n",
    "> - Display the first 5 rows from training dataset. (Hint: You can use the **head** function)\n",
    "Preprocessing: (10 pts)\n",
    "\n",
    "> - Check if there are any missing values in the dataset. If there are, you can either drop these values or fill it with most common values in corresponding rows. **Be careful that you have enough data for training the  model.**\n",
    "\n",
    "> - Encode categorical labels with the mappings given in the cell below. (Hint: You can use **map** function)\n",
    "\"\"\",\n",
    "\"\"\"Set X & y, split data (5 pts)\n",
    "\n",
    "*   Shuffle the dataset.\n",
    "*   Seperate your dependent variable X, and your independent variable y. The column health_metrics is y, the rest is X.\n",
    "*   Split training and test sets as 80% and 20%, respectively.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Features and Correlations (10 pts)\n",
    "\n",
    "* Correlations of features with health (4 points)\n",
    "Calculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap.\n",
    "\n",
    "* Feature Selection (3 points)\n",
    "Select a subset of features that are likely strong predictors, justifying your choices based on the computed correlations.\n",
    "\n",
    "* Hypothetical Driver Features (3 points)\n",
    "Propose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable.\n",
    "\n",
    "* __Note:__ You get can get help from GPT.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Tune Hyperparameters (20 pts)\n",
    "* Choose 2 hyperparameters to tune. You can use the Scikit learn decision tree documentation for the available hyperparameters *(Hyperparameters are listed under \"Parameters\" in the documentation)*. Use GridSearchCV for hyperparameter tuning, with a cross-validation value of 5. Use validation accuracy to pick the best hyper-parameter values. (15 pts)\n",
    "-Explain the hyperparameters you chose to tune. *(What are the hyperparameters you chose? Why did you choose them?)* (5 pts)\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Re-train and plot the decision tree with the hyperparameters you have chosen (15 pts)\n",
    "- Re-train model with the hyperparameters you have chosen in part 5). (10 pts)\n",
    "- Plot the tree you have trained. (5 pts)\n",
    "Hint: You can import the **plot_tree** function from the sklearn library.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Test your classifier on the test set (20 pts)\n",
    "- Predict the labels of testing data using the tree you have trained in step 6. (10 pts)\n",
    "- Report the classification accuracy. (2 pts)\n",
    "- Plot & investigate the confusion matrix. Fill the following blanks. (8 pts)\n",
    "> The model most frequently mistakes class(es) _________ for class(es) _________.\n",
    "Hint: You can use the confusion_matrix function from sklearn.metrics\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Find the information gain on the first split (10 pts)\"\"\",\n",
    "#####################\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the questions\n",
    "questions = [preprocess_text(question) for question in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorising with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = vectorizer.fit(prompts + questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_TF_IDF = pd.DataFrame(vectorizer.transform(questions).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "questions_TF_IDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TF-IDF model for each student and a dictionary that maps each student code to its\n",
    "# TF-IDF matrix (formed with the corresponding prompts)\n",
    "code2prompts_TF_IDF = dict()\n",
    "codes_with_no_prompts = []\n",
    "\n",
    "for code, user_prompts in code2prompts.items():\n",
    "    if len(user_prompts) == 0: # Excluding empty prompts and adding the codes associated with them to a list (to eliminate them from scores later)\n",
    "        print(code+\".html\")\n",
    "        codes_with_no_prompts.append(code)\n",
    "        continue\n",
    "    prompts_TF_IDF = pd.DataFrame(vectorizer.transform(user_prompts).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    code2prompts_TF_IDF[code] = prompts_TF_IDF\n",
    "\n",
    "print(code2prompts_TF_IDF[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].shape)\n",
    "code2prompts_TF_IDF[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the structure of the dictionary\n",
    "print(\"The user with code \" + list(code2prompts_TF_IDF.keys())[0] + \" has \" + str(len(code2prompts_TF_IDF[list(code2prompts_TF_IDF.keys())[0]])) + \" prompts.\")\n",
    "print(\"The first three prompts have the following TF-IDF matrices:\\n\" + str(code2prompts_TF_IDF[list(code2prompts_TF_IDF.keys())[0]].head(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that pairs each student code with the corresponding \n",
    "# cosine similarity (between prompts and questions) vector\n",
    "code2cosine = dict()\n",
    "for code, user_prompts_TF_IDF in code2prompts_TF_IDF.items():\n",
    "    code2cosine[code] = pd.DataFrame(cosine_similarity(questions_TF_IDF,user_prompts_TF_IDF))\n",
    "\n",
    "print(code2cosine[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].shape)\n",
    "code2cosine[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the highest cosine similarity value from each row (i.e. for each question) for each student and assigning the resulting vector to the student\n",
    "code2questionmapping = dict()\n",
    "for code, cosine_scores in code2cosine.items():\n",
    "    code2questionmapping[code] = code2cosine[code].max(axis=1).tolist()\n",
    "\n",
    "# Sample cosine similarity vector for a student\n",
    "print(code2questionmapping[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"])\n",
    "\n",
    "# Creating a dataframe that displays the mapping better\n",
    "question_mapping_scores = pd.DataFrame(code2questionmapping).T\n",
    "question_mapping_scores.reset_index(inplace=True)\n",
    "question_mapping_scores.rename(columns={i: f\"Q_{i}\" for i in range(len(questions))}, inplace=True)\n",
    "question_mapping_scores.rename(columns={\"index\": \"code\"}, inplace=True)\n",
    "\n",
    "# Displaying the dataframe\n",
    "print(question_mapping_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Outliers and Students with Empty Prompts (Preferred Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers [in this context]: Dummy or highly inconsistent prompts\n",
    "\n",
    "# Pipeline for detecting an outlier (an example):\n",
    "# Filtering the dataframe based on the desired code\n",
    "desired_code = \"6a2003ad-a05a-41c9-9d48-e98491a90499\"\n",
    "desired_file_scores = question_mapping_scores[question_mapping_scores['code'] == desired_code]\n",
    "\n",
    "# Calculating the average distance for the specific code\n",
    "average_distance = desired_file_scores.iloc[:, 1:].mean(axis=1).values[0]\n",
    "\n",
    "# Printing the average distance\n",
    "print(f\"The average distance for code {desired_code} is: {average_distance}\")\n",
    "\n",
    "threshold = 0.15\n",
    "\n",
    "question_mapping_scores['average_distance'] = question_mapping_scores.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "filtered_scores = question_mapping_scores[question_mapping_scores['average_distance'] >= threshold].reset_index(drop=True)\n",
    "\n",
    "filtered_scores = filtered_scores.drop(columns=['average_distance']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of rows in filtered_scores: {len(filtered_scores)}\")\n",
    "\n",
    "dropped_codes = question_mapping_scores[~question_mapping_scores['code'].isin(filtered_scores['code'])]['code'].tolist()\n",
    "\n",
    "# Print the codes of dropped entries\n",
    "print(\"Codes of dropped entries:\")\n",
    "print(dropped_codes)\n",
    "\n",
    "question_mapping_scores = filtered_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation with kNN (Alternative Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers [in this context]: Dummy or highly inconsistent prompts\n",
    "\n",
    "# Calculating the average distance for each code\n",
    "question_mapping_scores['average_distance'] = question_mapping_scores.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "# Setting the threshold for outliers\n",
    "threshold = 0.15\n",
    "\n",
    "# Identifying outliers\n",
    "outliers = question_mapping_scores[question_mapping_scores['average_distance'] < threshold]\n",
    "\n",
    "# Setting up the k-NN model\n",
    "k = 5  # Number of neighbors; you can adjust this number as needed\n",
    "nn_model = NearestNeighbors(n_neighbors=k+1)  # +1 because the point itself is included\n",
    "\n",
    "# Training the model with non-outlier data\n",
    "non_outliers = question_mapping_scores[question_mapping_scores['average_distance'] >= threshold]\n",
    "nn_model.fit(non_outliers.iloc[:, 1:-1])  # Exclude 'code' and 'average_distance' columns\n",
    "\n",
    "# For each outlier, find its nearest neighbors and replace its data\n",
    "for index, outlier in outliers.iterrows():\n",
    "    distances, indices = nn_model.kneighbors([outlier.iloc[1:-1]], n_neighbors=k+1)\n",
    "\n",
    "    # Exclude the first neighbor as it is the outlier itself\n",
    "    nearest_neighbors = non_outliers.iloc[indices[0][1:], 1:-1]\n",
    "\n",
    "    # Replace outlier data with the mean of its nearest neighbors\n",
    "    question_mapping_scores.loc[index, 1:-1] = nearest_neighbors.mean(axis=0)\n",
    "\n",
    "# Dropping the average_distance column as it's no longer needed\n",
    "question_mapping_scores = question_mapping_scores.drop(columns=['average_distance'])\n",
    "\n",
    "# Print the number of rows after processing\n",
    "print(f\"Number of rows after processing: {len(question_mapping_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorising with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF looks for direct word matches; word2vec tries to infer the context\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Creating a word2vec model using all prompts\n",
    "tokenized_prompts = [word_tokenize(prompt) for prompt in prompts if len(prompt) > 0]\n",
    "tokenized_questions = [word_tokenize(question) for question in questions if len(question) > 0]\n",
    "\n",
    "model = Word2Vec(tokenized_prompts + tokenized_questions, vector_size=500, window=5, min_count=1, workers=4)\n",
    "\n",
    "prompts_embeddings = [np.mean([model.wv[word] for word in prompt], axis=0) for prompt in tokenized_prompts]\n",
    "questions_embeddings = [np.mean([model.wv[word] for word in question], axis=0) for question in tokenized_questions]\n",
    "\n",
    "code2cosine_word2vec = dict()\n",
    "for code, prompt_embedding in zip(code2prompts.keys(), prompts_embeddings):\n",
    "    code2cosine_word2vec[code] = [cosine_similarity([question_embedding], [prompt_embedding])[0][0] for question_embedding in questions_embeddings]\n",
    "\n",
    "code2questionmapping_word2vec = pd.DataFrame(code2cosine_word2vec).T\n",
    "code2questionmapping_word2vec.reset_index(inplace=True, drop=False)\n",
    "code2questionmapping_word2vec.rename(columns={i: f\"Q_{i}\" for i in range(len(questions))}, inplace=True)\n",
    "code2questionmapping_word2vec.rename(columns={\"index\": \"code\"}, inplace=True)\n",
    "\n",
    "zero_length_prompts_codes = [code for code, prompt in code2prompts.items() if len(prompt) == 0]\n",
    "code2questionmapping_word2vec_filtered = code2questionmapping_word2vec[~code2questionmapping_word2vec['code'].isin(zero_length_prompts_codes)]\n",
    "\n",
    "question_mapping_scores = code2questionmapping_word2vec_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word2vec model for each student\n",
    "columns = [\"code\"] + [\"w\" + str(i) for i in range(500)]\n",
    "code2word2vec = pd.DataFrame(columns=columns)\n",
    "\n",
    "for code in code2prompts:\n",
    "    if len(code2prompts[code]) != 0:\n",
    "        model_w2v = Word2Vec(code2prompts[code], vector_size=500, window=5, min_count=1, workers=4)\n",
    "        \n",
    "        prompt_embeddings = []\n",
    "        for prompt in code2prompts[code]:\n",
    "            word_vectors = [model_w2v.wv[word] for word in prompt if word in model_w2v.wv]\n",
    "            if word_vectors:  # Checking if the list is not empty\n",
    "                average_vector = np.mean(word_vectors, axis=0)\n",
    "                prompt_embeddings.append(average_vector)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        if prompt_embeddings:\n",
    "            code_embedding = np.mean(prompt_embeddings, axis=0)\n",
    "\n",
    "            row = [code] + code_embedding.tolist()\n",
    "            code2word2vec = code2word2vec.append(pd.Series(row, index=columns), ignore_index=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(code2word2vec.shape)\n",
    "code2word2vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was normalized using Min-Max scaling. The normalized data was used only for neural network training; non-normalized data was used for other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the scores\n",
    "scores = pd.read_csv(\"Data/scores.csv\", sep=\",\")\n",
    "scores[\"code\"] = scores[\"code\"].apply(lambda x: x.strip())\n",
    "\n",
    "scores = scores[[\"code\", \"grade\"]]\n",
    "\n",
    "# Removing grades received by outlier students\n",
    "scores = scores[~scores[\"code\"].isin(dropped_codes)].reset_index(drop=True)\n",
    "scores = scores[~scores[\"code\"].isin(codes_with_no_prompts)].reset_index(drop=True)\n",
    "scores = scores[scores[\"grade\"] >= 70].reset_index(drop=True)\n",
    "\n",
    "# Displaying some scores\n",
    "print(scores.shape)\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of grades\n",
    "plt.title('Histogram Grades')\n",
    "plt.hist(scores[\"grade\"], rwidth=.8, bins=np.arange(min(scores[\"grade\"]), max(scores[\"grade\"])+2) - 0.5)\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scaling the scores\n",
    "scaled_values = scaler.fit_transform(scores[scores.columns[1:]])\n",
    "\n",
    "# Converting the scaled values back into a dataframe\n",
    "normalized_scores = pd.DataFrame(scaled_values, columns=scores.columns[1:])\n",
    "\n",
    "# Merging the 'code' column from the original 'scores' dataframe with the new 'normalized_scores'\n",
    "normalized_scores = pd.merge(scores[[\"code\"]], normalized_scores, left_index=True, right_index=True)\n",
    "\n",
    "print(normalized_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Possibly predictive features:\n",
    "* Number of prompts that a user asked\n",
    "* Number of complaints that a user made e.g \"the code gives this error!\"\n",
    "* The average number of characters in a user's prompts\n",
    "* Whether context was given in the first prompt\n",
    "* Number of apologies by ChatGPT\n",
    "* Number of ideal student keywords (words used by the best students)\n",
    "* Sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best students (those who scored 100)\n",
    "best_scores = scores[scores[\"grade\"] == 100]\n",
    "\n",
    "best_students = best_scores[\"code\"].tolist()\n",
    "print(len(best_students))\n",
    "print(\"The best students are: \" + str(best_students))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_of_best_students = [code2prompts[code] for code in best_students]\n",
    "\n",
    "# Finding words most frequently used by the best students\n",
    "counts = {}\n",
    "for prompts in prompts_of_best_students:\n",
    "    for prompt in prompts:\n",
    "        prompt_list = prompt.split()\n",
    "        for word in prompt_list:\n",
    "            if word not in counts:\n",
    "                counts[word] = 1\n",
    "            else:\n",
    "                counts[word] += 1\n",
    "\n",
    "# Sorting the words by their counts\n",
    "sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 words in best students' prompts: \" + str(sorted_counts[1:11]))\n",
    "\n",
    "sorted_keys = sorted(counts, key=counts.get, reverse=True)\n",
    "best_words = sorted_keys[1:11] #first one is num, so skip that\n",
    "print(\"Top 10 words in best students' prompts: \" + str(best_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_context_in_first_prompt(prompts):\n",
    "    if len(prompts) == 0:\n",
    "        return 0\n",
    "    \n",
    "    first_prompt= prompts[0]\n",
    "\n",
    "    context_words = [\"python\", \"machine learning\", \"computer science\", \"cs\", \"decision tree\", \"scikit-learn\", \"classifier\", \"classification\", \"classifiers\", \"classifications\", \"trees\", \"tree\", \"decision\"]\n",
    "\n",
    "    for word in context_words:\n",
    "        if word in first_prompt:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2features = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "keywords2search = [\"error\", \"no\", \"thank\", \"Entropy\"]\n",
    "keywords2search = [k.lower() for k in keywords2search]\n",
    "\n",
    "apology_keywords = [\"sorry\", \"apologize\", \"apology\", \"apologies\", \"oversight\"]\n",
    "apology_keywords = [k.lower() for k in apology_keywords]\n",
    "\n",
    "for code, convs in code2convos.items():\n",
    "    if len(convs) == 0:\n",
    "        print(code)\n",
    "        continue\n",
    "    for c in convs:\n",
    "        text = c[\"text\"].lower()\n",
    "        if c[\"role\"] == \"user\": # User\n",
    "            # Counting user prompts\n",
    "            code2features[code][\"#user_prompts\"] += 1\n",
    "\n",
    "            sentiment_score = TextBlob(text).sentiment.polarity\n",
    "            code2features[code][\"sentiment_score\"] += sentiment_score\n",
    "\n",
    "            # Counting the keywords\n",
    "            for kw in keywords2search:\n",
    "                code2features[code][f\"#{kw}\"] +=  len(re.findall(rf\"\\b{kw}\\b\", text))\n",
    "            \n",
    "            # Counting ideal words\n",
    "            for bw in best_words:\n",
    "                code2features[code][f\"#{bw}\"] +=  len(re.findall(rf\"\\b{bw}\\b\", text))\n",
    "\n",
    "            # Determining whether context was provided in the first prompt\n",
    "            code2features[code][\"context_in_first_prompt\"] = is_context_in_first_prompt(code2prompts[code])\n",
    "\n",
    "            code2features[code][\"prompt_avg_chars\"] += len(text)\n",
    "        else: # ChatGPT\n",
    "            code2features[code][\"response_avg_chars\"] += len(text)\n",
    "\n",
    "            # Counting apologies\n",
    "            for kw in apology_keywords:\n",
    "                code2features[code][f\"#{kw}\"] +=  len(re.findall(rf\"\\b{kw}\\b\", text))\n",
    "\n",
    "        code2features[code][\"prompt_avg_chars\"] /= code2features[code][\"#user_prompts\"]\n",
    "        code2features[code][\"response_avg_chars\"] /= code2features[code][\"#user_prompts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering the features in a dataframe\n",
    "df = pd.DataFrame(code2features).T\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=False)\n",
    "df.rename(columns={\"index\": \"code\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating mapping scores to the dataframe\n",
    "df = pd.merge(df, question_mapping_scores, on=\"code\", how=\"left\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Scores with Features\n",
    "4 datasets were created:\n",
    "* Engineereed features + highest cosine similarity for each question + regular scores\n",
    "* Engineered features + highest cosine similarity for each question + word2vec vector + regular scores\n",
    "* Engineered features + highest cosine similarity for each question + normalized scores\n",
    "* Engineered features + highest cosine similarity for each question + word2vec vector + normalized scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the newly created dataframe with regular scores\n",
    "temp_df_regular = pd.merge(df, scores, on=\"code\", how=\"left\")\n",
    "temp_df_regular.dropna(inplace=True)\n",
    "temp_df_regular.drop_duplicates(\"code\", inplace=True, keep=\"first\")\n",
    "temp_df_regular.head()\n",
    "\n",
    "temp_df_sorted_regular = temp_df_regular.sort_values(by=\"grade\", ascending=False)\n",
    "\n",
    "# Printing the sorted dataframe\n",
    "print(temp_df_sorted_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the newly created dataframe with separate word2vec models and regular scores\n",
    "temp_df_regular_extra_1 = pd.merge(df, code2word2vec, on=\"code\", how=\"left\")\n",
    "temp_df_regular_extra_1 = pd.merge(temp_df_regular_extra_1, scores, on=\"code\", how=\"left\")\n",
    "temp_df_regular_extra_1.dropna(inplace=True)\n",
    "temp_df_regular_extra_1.drop_duplicates(\"code\", inplace=True, keep=\"first\")\n",
    "temp_df_regular_extra_1.head()\n",
    "\n",
    "temp_df_sorted_regular_extra_1 = temp_df_regular_extra_1.sort_values(by=\"grade\", ascending=False)\n",
    "\n",
    "# Printing the sorted dataframe\n",
    "print(temp_df_sorted_regular_extra_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the newly created dataframe with normalized scores\n",
    "temp_df_normalized = pd.merge(df, normalized_scores, on=\"code\", how=\"left\")\n",
    "temp_df_normalized.dropna(inplace=True)\n",
    "temp_df_normalized.drop_duplicates(\"code\",inplace=True, keep=\"first\")\n",
    "temp_df_normalized.head()\n",
    "\n",
    "temp_df_sorted_normalized = temp_df_normalized.sort_values(by=\"grade\", ascending=False)\n",
    "\n",
    "# Printing the sorted dataframe\n",
    "print(temp_df_sorted_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the newly created dataframe separate word2vec models and normalized scores\n",
    "temp_df_normalized_extra_1 = pd.merge(df, code2word2vec, on=\"code\", how=\"left\")\n",
    "temp_df_normalized_extra_1 = pd.merge(temp_df_normalized_extra_1, normalized_scores, on=\"code\", how=\"left\")\n",
    "temp_df_normalized_extra_1.dropna(inplace=True)\n",
    "temp_df_normalized_extra_1.drop_duplicates(\"code\",inplace=True, keep=\"first\")\n",
    "temp_df_normalized_extra_1.head()\n",
    "\n",
    "temp_df_sorted_normalized_extra_1 = temp_df_normalized_extra_1.sort_values(by=\"grade\", ascending=False)\n",
    "\n",
    "# Printing the sorted dataframe\n",
    "print(temp_df_sorted_normalized_extra_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = temp_df_regular[temp_df_regular.columns[1:-1]].to_numpy()\n",
    "y = temp_df_regular[\"grade\"].to_numpy()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_extra_1 = temp_df_regular_extra_1[temp_df_regular_extra_1.columns[1:-1]].to_numpy()\n",
    "y_extra_1 = temp_df_regular_extra_1[\"grade\"].to_numpy()\n",
    "print(X_extra_1.shape, y_extra_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = temp_df_normalized[temp_df_normalized.columns[1:-1]].to_numpy()\n",
    "y_normalized = temp_df_normalized[\"grade\"].to_numpy()\n",
    "print(X_normalized.shape, y_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized_extra_1 = temp_df_normalized_extra_1[temp_df_normalized_extra_1.columns[1:-1]].to_numpy()\n",
    "y_normalized_extra_1 = temp_df_normalized_extra_1[\"grade\"].to_numpy()\n",
    "print(X_normalized_extra_1.shape, y_normalized_extra_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extra_1, X_test_extra_1, y_train_extra_1, y_test_extra_1 = train_test_split(X_extra_1, y_extra_1, test_size=0.2, random_state=42)\n",
    "print(\"Train set size:\", len(X_train_extra_1))\n",
    "print(\"Test set size:\", len(X_test_extra_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized, X_test_normalized, y_train_normalized, y_test_normalized = train_test_split(X_normalized, y_normalized, test_size=0.2, random_state=42)\n",
    "print(\"Train set size:\", len(X_train_normalized))\n",
    "print(\"Test set size:\", len(X_test_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized_extra_1, X_test_normalized_extra_1, y_train_normalized_extra_1, y_test_normalized_extra_1 = train_test_split(X_normalized_extra_1, y_normalized_extra_1, test_size=0.2, random_state=42)\n",
    "print(\"Train set size:\", len(X_train_normalized_extra_1))\n",
    "print(\"Test set size:\", len(X_test_normalized_extra_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "model_NN_1 = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model_NN_1.add(Dense(128, input_shape=(X_train_normalized.shape[1],), activation='relu'))\n",
    "\n",
    "# Hidden layers\n",
    "model_NN_1.add(Dense(64, activation='relu'))\n",
    "model_NN_1.add(Dropout(0.7))\n",
    "\n",
    "# Output layer\n",
    "model_NN_1.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "adam_optimizer = Adam(learning_rate=0.01)\n",
    "model_NN_1.compile(optimizer=adam_optimizer, loss='mean_squared_error', metrics=['mae', 'mse'])\n",
    "\n",
    "# Displaying the model summary\n",
    "model_NN_1.summary()\n",
    "\n",
    "# Training the model with a validation split\n",
    "history = model_NN_1.fit(\n",
    "    X_train_normalized, y_train_normalized,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_split=0.20,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "normalized_predictions = model_NN_1.predict(X_train_normalized)\n",
    "\n",
    "# Inverse transforming predictions and validation labels\n",
    "predictions = scaler.inverse_transform(normalized_predictions)\n",
    "y_valid_original = scaler.inverse_transform(y_train_normalized.reshape(-1, 1))\n",
    "\n",
    "# Calculating MAE and MSE on the original scale\n",
    "training_mae_1 = mean_absolute_error(y_valid_original, predictions)\n",
    "training_mse_1 = mean_squared_error(y_valid_original, predictions)\n",
    "\n",
    "print(f\"Mean absolute error: {training_mae_1}\")\n",
    "print(f\"Mean squared error: {training_mse_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_predictions = model_NN_1.predict(X_test_normalized)\n",
    "\n",
    "# Inverse transforming predictions and validation labels\n",
    "predictions = scaler.inverse_transform(normalized_predictions)\n",
    "y_valid_original = scaler.inverse_transform(y_test_normalized.reshape(-1, 1))\n",
    "\n",
    "# Calculating MAE and MSE on the original scale\n",
    "test_mae_1 = mean_absolute_error(y_valid_original, predictions)\n",
    "test_mse_1 = mean_squared_error(y_valid_original, predictions)\n",
    "\n",
    "print(f\"Mean absolute error: {test_mae_1}\")\n",
    "print(f\"Mean squared error: {test_mse_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "model_NN_2 = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model_NN_2.add(Dense(128, input_shape=(X_train_normalized_extra_1.shape[1],), activation='relu'))\n",
    "\n",
    "# Hidden layers\n",
    "model_NN_2.add(Dense(64, activation='relu'))\n",
    "model_NN_2.add(Dropout(0.7))\n",
    "\n",
    "# Output layer\n",
    "model_NN_2.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "adam_optimizer = Adam(learning_rate=0.01)\n",
    "model_NN_2.compile(optimizer=adam_optimizer, loss='mean_squared_error', metrics=['mae', 'mse'])\n",
    "\n",
    "# Displaying the model summary\n",
    "model_NN_2.summary()\n",
    "\n",
    "# Training the model with a validation split\n",
    "history = model_NN_2.fit(\n",
    "    X_train_normalized_extra_1, y_train_normalized_extra_1,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_split=0.20,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "normalized_predictions = model_NN_2.predict(X_train_normalized_extra_1)\n",
    "\n",
    "# Inverse transforming predictions and validation labels\n",
    "predictions = scaler.inverse_transform(normalized_predictions)\n",
    "y_valid_original = scaler.inverse_transform(y_train_normalized_extra_1.reshape(-1, 1))\n",
    "\n",
    "# Calculating MAE and MSE on the original scale\n",
    "training_mae_2 = mean_absolute_error(y_valid_original, predictions)\n",
    "training_mse_2 = mean_squared_error(y_valid_original, predictions)\n",
    "\n",
    "print(f\"Mean absolute error: {training_mae_2}\")\n",
    "print(f\"Mean squared error: {training_mse_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_predictions = model_NN_2.predict(X_test_normalized_extra_1)\n",
    "\n",
    "# Inverse transforming predictions and validation labels\n",
    "predictions = scaler.inverse_transform(normalized_predictions)\n",
    "y_valid_original = scaler.inverse_transform(y_test_normalized_extra_1.reshape(-1, 1))\n",
    "\n",
    "# Calculating MAE and MSE on the original scale\n",
    "test_mae_2 = mean_absolute_error(y_valid_original, predictions)\n",
    "test_mse_2 = mean_squared_error(y_valid_original, predictions)\n",
    "\n",
    "print(f\"Mean absolute error: {test_mae_2}\")\n",
    "print(f\"Mean squared error: {test_mse_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of k values to try\n",
    "k_values = range(2, len(set(scores[\"grade\"])) + 1)  # Adjust this range based on your dataset\n",
    "\n",
    "sum_of_squared_distances = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X_extra_1)\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow Graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Sum of squared distances\")\n",
    "plt.title(\"Elbow Method For Optimal k\")\n",
    "plt.xticks(k_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_temp_df_regular_extra_1 = temp_df_regular_extra_1\n",
    "\n",
    "# Choosing the number of clusters\n",
    "num_clusters = 6\n",
    "\n",
    "# Applying K-Means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "clusters = kmeans.fit_predict(X_extra_1)\n",
    "\n",
    "# Adding the cluster labels to the newly created replicate dataframe\n",
    "clustered_temp_df_regular_extra_1['cluster'] = clusters\n",
    "\n",
    "clustered_temp_df_regular_extra_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor(random_state=0,criterion='squared_error', max_depth=10)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "################################################################\n",
    "##############       HYPERPARAMETER TUNING      ################\n",
    "################################################################\n",
    "################################################################\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# setup parameter space\n",
    "parameters = {'criterion':['squared_error'],\n",
    "              'max_depth':np.arange(1,21).tolist()[0::2],\n",
    "              'min_samples_split':np.arange(2,11).tolist()[0::2],\n",
    "              'max_leaf_nodes':np.arange(3,26).tolist()[0::2]}\n",
    "\n",
    "# create an instance of the grid search object\n",
    "g2 = GridSearchCV(DecisionTreeRegressor(), parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# conduct grid search over the parameter space\n",
    "start_time = time.time()\n",
    "g2.fit(X_train,y_train)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# show best parameter configuration found for regressor\n",
    "rgr_params1 = g2.best_params_\n",
    "rgr_params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_MSEs = regressor.tree_.impurity   \n",
    "for idx, MSE in enumerate(regressor.tree_.impurity):\n",
    "    print(\"Node {} has MSE {}\".format(idx,MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Tree \n",
    "dot_data = tree.export_graphviz(regressor, out_file=None, feature_names=temp_df_regular.columns[1:-1])\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"hw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculation of Mean Absolute Error (MAE), Mean Squared Error (MSE), and R2 score\n",
    "training_mae_3 = mean_squared_error(y_train, y_train_pred)\n",
    "test_mae_3 = mean_absolute_error(y_test, y_test_pred)\n",
    "training_mse_3 = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mse_3 = mean_squared_error(y_test, y_test_pred) \n",
    "training_r2_score_1 = r2_score(y_train, y_train_pred)\n",
    "test_r2_score_1 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {training_mse_3}\")\n",
    "print(f\"Test MSE: {test_mse_3}\")\n",
    "print(f\"Training MAE: {training_mae_3}\")\n",
    "print(f\"Test MAE: {test_mae_3}\")\n",
    "\n",
    "print(f\"Training R2: {training_r2_score_1}\")\n",
    "print(f\"Test R2: {test_r2_score_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "################################################################\n",
    "##########         RandomForestRegressor APPROACH      #########\n",
    "################################################################\n",
    "################################################################\n",
    "\n",
    "# Extracting features (X) and target variable (y)\n",
    "X = temp_df_regular[temp_df_regular.columns[1:-1]].to_numpy()\n",
    "y = temp_df_regular[\"grade\"].to_numpy()\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))\n",
    "\n",
    "# Setting up the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initializing the RandomForestRegressor\n",
    "regressor = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# Creating the GridSearchCV object\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fitting the model with the grid search parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Making predictions on the test set using the best model\n",
    "best_regressor = grid_search.best_estimator_\n",
    "test_predictions = best_regressor.predict(X_test)\n",
    "train_predictions = best_regressor.predict(X_train)\n",
    "\n",
    "# Evaluating the model\n",
    "training_mae_4 = mean_absolute_error(y_train, train_predictions)\n",
    "test_mae_4 = mean_absolute_error(y_test, test_predictions)\n",
    "training_mse_4 = mean_squared_error(y_train, train_predictions)\n",
    "test_mse_4 = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"Training MAE: {training_mae_4}\")\n",
    "print(f\"Test MAE: {test_mae_4}\")\n",
    "print(f\"Training MSE: {training_mse_4}\")\n",
    "print(f\"Test MSE: {test_mse_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "################################################################\n",
    "#############          XGBOOST APPROACH           ##############\n",
    "################################################################\n",
    "################################################################\n",
    "\n",
    "X = temp_df_regular.drop(columns=['grade'])  # Features\n",
    "y = temp_df_regular['grade']  # Target variable\n",
    "\n",
    "print(temp_df_regular.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.drop(columns=['code'])\n",
    "X_test = X_test.drop(columns=['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MSE Train:\", mean_squared_error(y_train,model.predict(X_train)))\n",
    "print(\"MSE TEST:\", mean_squared_error(y_test,y_pred))\n",
    "\n",
    "print(\"R2 Train:\", r2_score(y_train,model.predict(X_train)))\n",
    "print(\"R2 TEST:\", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Printing the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Making predictions using the best model\n",
    "final_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluating the best model\n",
    "mae = mean_absolute_error(y_test, final_predictions)\n",
    "mse = mean_squared_error(y_test, final_predictions)\n",
    "r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = temp_df_regular.drop(columns=['grade'])  # Features\n",
    "y = temp_df_regular['grade']  # Target variable\n",
    "\n",
    "# Droppinf 'code' column before splitting\n",
    "X = X.drop(columns=['code'])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# K-fold cross-validation\n",
    "cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Displaying cross-validation results\n",
    "print(\"Cross-validation Results:\")\n",
    "print(\"Negative Mean Squared Errors:\", cv_results)\n",
    "print(\"Average Negative Mean Squared Error:\", cv_results.mean())\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Printing the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Making predictions using the best model\n",
    "test_final_predictions = best_model.predict(X_test)\n",
    "train_final_predictions = best_model.predict(X_train)\n",
    "\n",
    "# Evaluating the best model\n",
    "training_mae_5 = mean_absolute_error(y_train, train_final_predictions)\n",
    "test_mae_5 = mean_absolute_error(y_test, test_final_predictions)\n",
    "training_mse_5 = mean_squared_error(y_train, train_final_predictions)\n",
    "test_mse_5 = mean_squared_error(y_test, test_final_predictions)\n",
    "test_r2_score_2 = r2_score(y_test, test_final_predictions)\n",
    "\n",
    "print(f\"Training MAE: {training_mae_5}\")\n",
    "print(f\"Test MAE: {test_mae_5}\")\n",
    "print(f\"Training MSE: {training_mse_5}\")\n",
    "print(f\"Test MSE: {test_mse_5}\")\n",
    "print(f\"Test R2 score: {test_r2_score_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_above_70 = temp_df_regular[temp_df_regular[\"grade\"] >= 70]\n",
    "\n",
    "X = temp_df_above_70[temp_df_above_70.columns[1:-1]].to_numpy()\n",
    "y = temp_df_above_70[\"grade\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculating the mean of training set grades\n",
    "average_grade = y_train.mean()\n",
    "\n",
    "# Assigning the mean grade as the prediction for all instances in the test set\n",
    "test_constant_predictions = np.full_like(y_test, fill_value=average_grade)\n",
    "train_constant_predictions = np.full_like(y_train, fill_value=average_grade)\n",
    "\n",
    "# Evaluating the constant prediction\n",
    "training_mae_6 = mean_absolute_error(y_train, train_constant_predictions)\n",
    "training_mse_6 = mean_squared_error(y_train, train_constant_predictions)\n",
    "test_mae_6 = mean_absolute_error(y_test, test_constant_predictions)\n",
    "test_mse_6 = mean_squared_error(y_test, test_constant_predictions)\n",
    "test_r2_score_3 = r2_score(y_test, test_constant_predictions)\n",
    "\n",
    "# Printing evaluation metrics for the constant prediction using mean\n",
    "print(f\"Training MAE: {training_mae_6}\")\n",
    "print(f\"Test MAE: {test_mae_6}\")\n",
    "print(f\"Training MSE: {training_mse_6}\")\n",
    "print(f\"Test MSE: {test_mse_6}\")\n",
    "print(f\"Test R2 score: {test_r2_score_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the median of training set grades\n",
    "median_grade = np.median(y_train)\n",
    "\n",
    "# Assigning the median grade as the prediction for all instances in the test set\n",
    "test_constant_predictions_median = np.full_like(y_test, fill_value=median_grade)\n",
    "train_constant_predictions_median = np.full_like(y_train, fill_value=median_grade)\n",
    "\n",
    "# Evaluating the constant prediction\n",
    "training_mae_7 = mean_absolute_error(y_train, train_constant_predictions_median)\n",
    "training_mse_7 = mean_squared_error(y_train, train_constant_predictions_median) \n",
    "test_mae_7 = mean_absolute_error(y_test, test_constant_predictions_median)\n",
    "test_mse_7 = mean_squared_error(y_test, test_constant_predictions_median)\n",
    "test_r2_score_4 = r2_score(y_test, test_constant_predictions_median)\n",
    "\n",
    "# Printing evaluation metrics for the constant prediction using mean\n",
    "print(f\"Training MAE: {training_mae_7}\")\n",
    "print(f\"Test MAE: {test_mae_7}\")\n",
    "print(f\"Training MSE: {training_mse_7}\")\n",
    "print(f\"Test MSE: {test_mse_7}\")\n",
    "print(f\"Test R2 score: {test_r2_score_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes = {\"Neural Network 1 (W2V vector is excluded)\": {\"Training\": training_mae_1, \"Test\":  test_mae_1},\n",
    "        \"Neural Network 2 (W2V vector is included)\": {\"Training\": training_mae_2, \"Test\": test_mae_2},\n",
    "        \"Decision Tree\": {\"Training\": training_mae_3, \"Test\": test_mae_3},\n",
    "        \"Random Forest\": {\"Training\": training_mae_4, \"Test\": test_mae_4},\n",
    "        \"XGBoost\": {\"Training\": training_mae_5, \"Test\": test_mae_5},\n",
    "        \"Mean (Dummy Regressor)\": {\"Training\": training_mae_6, \"Test\": test_mae_6},\n",
    "        \"Median (Dummy Regressor)\": {\"Training\": training_mae_7, \"Test\": test_mae_7}}\n",
    "\n",
    "mses = {\"Neural Network 1 (W2V vector is excluded)\": {\"Training\": training_mse_1, \"Test\":  test_mse_1},\n",
    "        \"Neural Network 2 (W2V vector is included)\": {\"Training\": training_mse_2, \"Test\": test_mse_2},\n",
    "        \"Decision Tree\": {\"Training\": training_mse_3, \"Test\": test_mse_3},\n",
    "        \"Random Forest\": {\"Training\": training_mse_4, \"Test\": test_mse_4},\n",
    "        \"XGBoost\": {\"Training\": training_mse_5, \"Test\": test_mse_5},\n",
    "        \"Mean (Dummy Regressor)\": {\"Training\": training_mse_6, \"Test\": test_mse_6},\n",
    "        \"Median (Dummy Regressor)\": {\"Training\": training_mse_7, \"Test\": test_mse_7}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, title):\n",
    "    models = list(metrics.keys())\n",
    "    training_errors = [metrics[model]['Training'] for model in models]\n",
    "    test_errors = [metrics[model]['Test'] for model in models]\n",
    "\n",
    "    # Setting the positions and width for the bars\n",
    "    pos = np.arange(len(models))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(pos, training_errors, bar_width, label='Training', alpha=0.7)\n",
    "    plt.bar(pos + bar_width, test_errors, bar_width, label='Test', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(title)\n",
    "    plt.xticks(pos + bar_width / 2, models, rotation=45, ha=\"right\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(maes, \"Mean Absolute Error (MAE) by Model\")\n",
    "plot_metrics(mses, \"Mean Squared Error (MSE) by Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting \"grade\" and \"cluster\" for the plot\n",
    "plot_data = clustered_temp_df_regular_extra_1[[\"grade\", \"cluster\"]]\n",
    "\n",
    "# Creating a new column for y-axis to represent clusters distinctly\n",
    "plot_data[\"y_random\"] = np.random.rand(len(plot_data))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=\"grade\", y=\"y_random\", hue=\"cluster\", data=plot_data, palette=\"bright\", legend=\"full\", s=100)  # s is the size of points\n",
    "plt.title(\"Results of Clustering\")\n",
    "plt.xlabel(\"Grade\")\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks([])  # Hiding y-axis ticks\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc=2)  # Moving the legend outside the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAEs of our models: \")\n",
    "pprint(maes)\n",
    "\n",
    "print(\"MSEs of our models: \")\n",
    "pprint(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model with the best MAE in training:\")\n",
    "mae_training_min = 100\n",
    "model_name = \"\"\n",
    "for model in maes.keys():\n",
    "    if maes[model][\"Training\"] < mae_training_min:\n",
    "        mae_training_min = maes[model][\"Training\"]\n",
    "        model_name = model\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "print(\"Model with the best MAE in test:\")\n",
    "mae_test_min = 100\n",
    "model_name = \"\"\n",
    "for model in maes.keys():\n",
    "    if maes[model][\"Test\"] < mae_test_min:\n",
    "        mae_test_min = maes[model][\"Test\"]\n",
    "        model_name = model\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "print(\"Model with the best MSE in training:\")\n",
    "mse_training_min = 100\n",
    "model_name = \"\"\n",
    "for model in mses.keys():\n",
    "    if mses[model][\"Training\"] < mse_training_min:\n",
    "        mse_training_min = mses[model][\"Training\"]\n",
    "        model_name = model\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "print(\"Model with the best MSE in test:\")\n",
    "mse_test_min = 100\n",
    "model_name = \"\"\n",
    "for model in mses.keys():\n",
    "    if mses[model][\"Test\"] < mse_test_min:\n",
    "        mse_test_min = mses[model][\"Test\"]\n",
    "        model_name = model\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
